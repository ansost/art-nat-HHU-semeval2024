{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to fine-tune the RoBERTa Open AI detector model with one tenth of the trianing data from task A. It is assumed that this code is run in Google Colab. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "The code below was used to generate a split of the dataset. The dataset is available at XXXXXX. \n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "def get_random_samples(data, model, size):\n",
    "    content = data[\"model\"] == model\n",
    "    random_samples = data[content].sample(n=size)\n",
    "\n",
    "    return random_samples\n",
    "\n",
    "data_path = \"subtaskA_train_monolingual.jsonl\"\n",
    "data = pd.read_json(path_or_buf=data_path, lines=True)\n",
    "unique_model = set(data[\"model\"])\n",
    "random_samples_2000_per_model = []\n",
    "\n",
    "for model in unique_model:\n",
    "  samples = get_random_samples(data, model, 2000)\n",
    "  random_samples_2000_per_model.append(samples)\n",
    "\n",
    "sample_set_10000 = pd.concat(random_samples_2000_per_model, ignore_index=True)\n",
    "sample_set_10000.to_json(\"sample_set_10000.json\", orient=\"records\", lines=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning\n",
    "A Huggingface account and access token are required to execute the code below. In your account, go to Settings > Access Tokens to retrieve it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install pandas\n",
    "!pip install evaluate\n",
    "!pip install numpy\n",
    "!pip install transformers\n",
    "!pip install -U scikit-learn\n",
    "!pip install scipy\n",
    "!pip install tensorflow==2.14 # Upgrade to higher version of Tensorflow (standard with Colab is 12.0)\n",
    "!pip install accelerate -U #  Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoTokenizer,\n",
    "    set_seed,\n",
    ")\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.special import softmax\n",
    "import argparse\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"roberta-base-openai-detector\"\n",
    "train_file_path = \"/content/drive/MyDrive/test folder/sample_set_10000.jsonl\"\n",
    "test_file_path = \"/content/drive/MyDrive/test folder/subtaskA_dev_monolingual.jsonl\"\n",
    "id2label = {0: \"human\", 1: \"machine\"}\n",
    "label2id = {\"human\": 0, \"machine\": 1}\n",
    "random_seed = 0\n",
    "\n",
    "# If you want to save to directory rather than Hugging Face\n",
    "checkpoints_path = \"\"  # If you want to save to directory rather than Hugging Face\n",
    "best_model_path = \"\"  # If you want to save to directory rather than Hugging Face\n",
    "output_path = \"artificially-natural-roberta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, **fn_kwargs):\n",
    "    return fn_kwargs[\"tokenizer\"](examples[\"text\"], truncation=True)\n",
    "\n",
    "\n",
    "def get_data(train_path, test_path, random_seed):\n",
    "    train_df = pd.read_json(train_path, lines=True)\n",
    "    test_df = pd.read_json(test_path, lines=True)\n",
    "    train_df, val_df = train_test_split(\n",
    "        train_df, test_size=0.2, stratify=train_df[\"label\"], random_state=random_seed\n",
    "    )\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    results = {}\n",
    "    results.update(\n",
    "        f1_metric.compute(predictions=predictions, references=labels, average=\"micro\")\n",
    "    )\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_tokenizer_and_model(model_name, id2label, label2id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=len(label2id), id2label=id2label, label2id=label2id\n",
    "    )\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "def get_data_collator(tokenizer):\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    return data_collator\n",
    "\n",
    "\n",
    "def prepare_data_for_training(train_df, valid_df, tokenizer):\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    valid_dataset = Dataset.from_pandas(valid_df)\n",
    "    tokenized_train_dataset = train_dataset.map(\n",
    "        preprocess_function, batched=True, fn_kwargs={\"tokenizer\": tokenizer}\n",
    "    )\n",
    "    tokenized_valid_dataset = valid_dataset.map(\n",
    "        preprocess_function, batched=True, fn_kwargs={\"tokenizer\": tokenizer}\n",
    "    )\n",
    "    return tokenized_train_dataset, tokenized_valid_dataset\n",
    "\n",
    "\n",
    "def create_trainer(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    data_collator,\n",
    "    tokenized_train_dataset,\n",
    "    tokenized_valid_dataset,\n",
    "    output_dir_path,\n",
    "):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir_path,\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=True,\n",
    "        # hub_model_id =\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_valid_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    return trainer\n",
    "\n",
    "\n",
    "def finetune_and_save_best(trainer):\n",
    "    trainer.train()\n",
    "    trainer.push_to_hub(\"End of training.\")\n",
    "    # trainer.save_model(best_model_path) # Enable if you want to save to local directory\n",
    "\n",
    "\n",
    "def test(test_df, model_path, id2label, label2id):\n",
    "\n",
    "    tokenizer, model = get_tokenizer_and_model(model_path, id2label, label2id)\n",
    "\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "    tokenized_test_dataset = test_dataset.map(\n",
    "        preprocess_function, batched=True, fn_kwargs={\"tokenizer\": tokenizer}\n",
    "    )\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    predictions = trainer.predict(tokenized_test_dataset)\n",
    "    prob_pred = softmax(predictions.predictions, axis=-1)\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "    metric = evaluate.load(\"bstrai/classification_report\")\n",
    "    results = metric.compute(predictions=preds, references=predictions.label_ids)\n",
    "    return results, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(random_seed)\n",
    "\n",
    "train_df, valid_df, test_df = get_data(train_file_path, test_file_path, random_seed)\n",
    "tokenizer, model = get_tokenizer_and_model(model_name, id2label, label2id)\n",
    "data_collator = get_data_collator(tokenizer)\n",
    "tokenized_train_dataset, tokenized_valid_dataset = prepare_data_for_training(\n",
    "    train_df, valid_df, tokenizer\n",
    ")\n",
    "\n",
    "trainer = create_trainer(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    data_collator,\n",
    "    tokenized_train_dataset,\n",
    "    tokenized_valid_dataset,\n",
    "    output_path,\n",
    ")\n",
    "\n",
    "finetune_and_save_best(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valiation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_file_path = \"subtaskA_dev_monolingual.jsonl\"\n",
    "checkpoint_for_testing = \"artificially-natural-roberta-redone\"\n",
    "prediction_file_path = \"new_predictions.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_df = pd.read_json(t_test_file_path, lines=True)\n",
    "results, predictions = test(t_test_df, checkpoint_for_testing, id2label, label2id)\n",
    "print(results, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predictions to folder in this notebook\n",
    "predictions_df = pd.DataFrame({\"id\": test_df[\"id\"], \"label\": predictions})\n",
    "predictions_df.to_json(prediction_file_path, lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission predictions from test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_test_file_path = \"test_subtaskA_monolingual.jsonl\"\n",
    "finetuned_model_id = \"artificially-natural-roberta\"\n",
    "submission_prediction_file_path = \"subtask_a_monolingual.jsonl\"\n",
    "submission_id2label = {0: \"human\", 1: \"machine\"}\n",
    "submission_label2id = {\"human\": 0, \"machine\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission_get_and_prepare_data(test_path, tokenizer, model):\n",
    "    test_df = pd.read_json(test_path, lines=True)\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "    tokenized_test_dataset = test_dataset.map(\n",
    "        preprocess_function, batched=True, fn_kwargs={\"tokenizer\": tokenizer}\n",
    "    )\n",
    "    return tokenized_test_dataset\n",
    "\n",
    "\n",
    "def submission_predict(tokenized_test_df, tokenizer, model, prediction_file_path):\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    predictions = trainer.predict(tokenized_test_df)\n",
    "    prob_pred = softmax(predictions.predictions, axis=-1)\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "    return predictions, preds\n",
    "\n",
    "\n",
    "submission_tokenizer, submission_model = get_tokenizer_and_model(\n",
    "    finetuned_model_id, submission_id2label, submission_label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_input = submission_get_and_prepare_data(\n",
    "    submission_test_file_path, submission_tokenizer, submission_model\n",
    ")\n",
    "sub_predictions, sub_preds = submission_predict(\n",
    "    submission_input,\n",
    "    submission_tokenizer,\n",
    "    submission_model,\n",
    "    submission_prediction_file_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame({\"id\": submission_input[\"id\"], \"label\": sub_preds})\n",
    "predictions_df.to_json(submission_prediction_file_path, lines=True, orient=\"records\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taskAV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
